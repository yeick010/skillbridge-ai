# Setup
!pip install langchain langchain-huggingface faiss-cpu transformers replicate

# Imposta API Token Replicate
from dotenv import load_dotenv
load_dotenv()

import os
replicate_token = os.getenv("REPLICATE_API_TOKEN")
os.environ["REPLICATE_API_TOKEN"] = replicate_token


#  Knowledge base con skill
texts = [
    "Un Data Scientist nel settore assicurativo deve conoscere la modellazione del rischio e la rilevazione delle frodi.",
    "L'analisi predittiva Ã¨ usata per prevedere sinistri e ottimizzare i premi.",
    "Skill avanzate includono NLP per l'analisi di documenti di polizza.",
    "Machine Learning e Python sono fondamentali per l'automazione dei processi.",
    "La conoscenza dei regolamenti assicurativi migliora l'interpretazione dei dati."
]

# Embedding con Granite + LangChain

from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embeddings_model = HuggingFaceEmbeddings(
    model_name="ibm-granite/granite-embedding-30m-english"
)

db = FAISS.from_texts(texts, embedding=embeddings_model)

# Retrieval + LLM Granite (via Replicate)

from langchain.retrievers import VectorStoreRetriever
from langchain.chains import RetrievalQA
from langchain.llms import Replicate

llm = Replicate(
    model="ibm/granite-3b-8-instruct",
    model_kwargs={"temperature": 0.7, "top_p": 0.95}
)

retriever = VectorStoreRetriever(vectorstore=db)

rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

 # Fai una domanda sulle skill

query = "Quali competenze dovrei sviluppare se voglio specializzarmi in AI assicurativa?"
result = rag_chain({"query": query})
print("ðŸ§  Risposta generata:\n")
print(result["result"])
